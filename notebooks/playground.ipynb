{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "TORCH_USE_CUDA_DSA=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = '/home/smbh/iirbctf/data/clevr/CLEVR_CoGenT_v1.0/'\n",
    "x_train_img = []\n",
    "for img in os.listdir(dataset_base_path + 'images/trainA'):\n",
    "    x_train_img.append(img)\n",
    "x_valid_img = []\n",
    "for img in os.listdir(dataset_base_path + 'images/valA'):\n",
    "    x_valid_img.append(img)\n",
    "x_test_img = []\n",
    "for img in os.listdir(dataset_base_path + 'images/testA'):\n",
    "    x_test_img.append(img)\n",
    "\n",
    "y_train_img = json.load(open(dataset_base_path + \"scenes/CLEVR_trainA_scenes.json\", \"r\"))['scenes']\n",
    "y_valid_img = json.load(open(dataset_base_path + \"scenes/CLEVR_valA_scenes.json\", \"r\"))['scenes']\n",
    "\n",
    "\n",
    "print('x_train_img: ', len(x_train_img))\n",
    "print('x_valid_img: ', len(x_valid_img))\n",
    "print('x_test_img: ', len(x_test_img))\n",
    "\n",
    "print('y_train_img: ', len(y_train_img))\n",
    "print('y_valid_img: ', len(y_valid_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_txt = json.load(open(dataset_base_path + \"/questions/CLEVR_trainA_questions.json\", \"r\"))['questions']\n",
    "x_valid_txt = json.load(open(dataset_base_path + \"/questions/CLEVR_valA_questions.json\", \"r\"))['questions']\n",
    "\n",
    "\n",
    "print('x_train_txt: ', len(x_train_txt))\n",
    "print('x_valid_txt: ', len(x_valid_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROPORTION = 0.01\n",
    "# DATA_PROPORTION = 1\n",
    "\n",
    "x_train_img = x_train_img[:int(DATA_PROPORTION * len(x_train_img))]\n",
    "x_valid_img = x_valid_img[:int(DATA_PROPORTION * len(x_valid_img))]\n",
    "x_test_img = x_test_img[:int(DATA_PROPORTION * len(x_test_img))]\n",
    "\n",
    "print('x_train_img: ', len(x_train_img))\n",
    "print('x_valid_img: ', len(x_valid_img))\n",
    "print('x_test_img: ', len(x_test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_PROPORTION != 1:\n",
    "    y_train_img = [y for y in y_train_img if y['image_filename'] in x_train_img]\n",
    "    y_valid_img = [y for y in y_valid_img if y['image_filename'] in x_valid_img]\n",
    "\n",
    "    x_train_txt = [x for x in x_train_txt if x['image_filename'] in x_train_img]\n",
    "    x_valid_txt = [x for x in x_valid_txt if x['image_filename'] in x_valid_img]\n",
    "\n",
    "print('y_train_img: ', len(y_train_img))\n",
    "print('y_valid_img: ', len(y_valid_img))\n",
    "\n",
    "print('x_train_txt: ', len(x_train_txt))\n",
    "print('x_valid_txt: ', len(x_valid_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(x_valid_img))\n",
    "img = cv2.imread(dataset_base_path + 'images/valA/' + x_valid_img[idx])\n",
    "INPUT_SHAPE = img.shape\n",
    "print(INPUT_SHAPE)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_txt[random.randint(0, len(x_train_img))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid_img[0].keys())\n",
    "print(\"----\" * 4)\n",
    "print(\"Image filename: \", y_valid_img[0]['image_filename'])\n",
    "print(\"Split: \", y_valid_img[0]['split'])\n",
    "print(\"Image index: \", y_valid_img[0]['image_index'])\n",
    "print(\"----\" * 4)\n",
    "print(\"Objects: \")\n",
    "for k, v in y_valid_img[0]['objects'][0].items():\n",
    "    print(\"\\t\", k, v)\n",
    "print(\"----\" * 4)\n",
    "print(\"Relationships: \")\n",
    "for k, v in y_valid_img[0]['relationships'].items():\n",
    "    print(\"\\t\", k, v)\n",
    "print(\"----\" * 4)\n",
    "print(\"Directions: \")\n",
    "for k, v in y_valid_img[0]['directions'].items():\n",
    "    print(\"\\t\", k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"<PAD>\": 0}\n",
    "x_train_questions = []\n",
    "y_train_answers = []\n",
    "x_valid_questions = []\n",
    "y_valid_answers = []\n",
    "MAX_LENGTH = 0\n",
    "\n",
    "\n",
    "def vocab_counter(dataset, vocab, MAX_LENGTH):\n",
    "    for q in dataset:\n",
    "        _q = q[\"question\"].replace(\"?\", \"\").replace(\".\", \"\").replace(\",\", \"\").split(\" \")\n",
    "        x_train_questions.append(_q)\n",
    "        y_train_answers.append(q[\"answer\"])\n",
    "        if len(_q) > MAX_LENGTH:\n",
    "            MAX_LENGTH = len(_q)\n",
    "        # question\n",
    "        for w in _q:\n",
    "            if w in vocab:\n",
    "                vocab[w] += 1\n",
    "            else:\n",
    "                vocab[w] = 1\n",
    "        # answer\n",
    "        for w in q[\"answer\"].split(\" \"):\n",
    "            if w in vocab:\n",
    "                vocab[w] += 1\n",
    "            else:\n",
    "                vocab[w] = 1\n",
    "        # inputs\n",
    "        for p in q[\"program\"]:\n",
    "            for _iv in p[\"value_inputs\"]:\n",
    "                if _iv in vocab:\n",
    "                    vocab[_iv] += 1\n",
    "                else:\n",
    "                    vocab[_iv] = 1\n",
    "            # functions\n",
    "            for w in p[\"function\"].split(\"_\"):\n",
    "                if w in vocab:\n",
    "                    vocab[w] += 1\n",
    "                else:\n",
    "                    vocab[w] = 1\n",
    "    return vocab, MAX_LENGTH\n",
    "\n",
    "vocab, MAX_LENGTH = vocab_counter(x_train_txt, vocab, MAX_LENGTH)\n",
    "vocab, MAX_LENGTH = vocab_counter(x_valid_txt, vocab, MAX_LENGTH)\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "vocab.remove(\"\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input sentences to a tensor of indices\n",
    "def encode_sentence(sentence, vocab, MAX_LENGTH):\n",
    "    sentence = sentence.replace(\"?\", \"\").replace(\".\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    sentence = [w for w in sentence if w != \"\"]\n",
    "    sentence = sentence + [\"<PAD>\"] * (MAX_LENGTH - len(sentence))\n",
    "    encoded = []\n",
    "    for w in sentence:\n",
    "        encoded.append(vocab.index(w))\n",
    "    return encoded\n",
    "\n",
    "def encode_answer(answer, vocab):\n",
    "    encoded = []\n",
    "    for w in answer.split(\" \"):\n",
    "        encoded.append(vocab.index(w))\n",
    "    return encoded\n",
    "\n",
    "y_train_txt = [encode_answer(q[\"answer\"], vocab) for q in x_train_txt]\n",
    "y_valid_txt = [encode_answer(q[\"answer\"], vocab) for q in x_valid_txt]\n",
    "\n",
    "x_train_txt = [encode_sentence(q[\"question\"], vocab, MAX_LENGTH) for q in x_train_txt]\n",
    "x_valid_txt = [encode_sentence(q[\"question\"], vocab, MAX_LENGTH) for q in x_valid_txt]\n",
    "\n",
    "\n",
    "print(\"x_train_txt: \", len(x_train_txt))\n",
    "print(\"y_train_txt: \", len(y_train_txt))\n",
    "\n",
    "print(\"x_valid_txt: \", len(x_valid_txt))\n",
    "print(\"y_valid_txt: \", len(y_valid_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.query_layer = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_layer = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_layer = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.sqrt_dv = np.sqrt(feature_dim)\n",
    "\n",
    "    def forward(self, V):\n",
    "        Q = self.query_layer(V)\n",
    "        K = self.key_layer(V)\n",
    "        V = self.value_layer(V)\n",
    "\n",
    "        # permute dimensions for matrix multiplication\n",
    "        Q = Q.permute(0, 2, 1)\n",
    "        K = K.permute(0, 2, 1)\n",
    "        V = V.permute(0, 2, 1)\n",
    "        \n",
    "        attention_weights = F.softmax(Q @ K / self.sqrt_dv, dim=-1)\n",
    "\n",
    "        # add identity matrix\n",
    "        attention_weights += torch.eye(V.size(1)).to(V.device)\n",
    "        \n",
    "        # V = attention_weights @ V\n",
    "        V = attention_weights @ V.permute(0, 2, 1)  # [batch_size, seq_length, feature_dim]\n",
    "\n",
    "        return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a pre-trained Faster R-CNN model\n",
    "# /home/smbh/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_attention = Attention(feature_dim=256).to(device)\n",
    "\n",
    "# put the model in evaluation mode and move to the appropriate device\n",
    "visual_attention.eval()\n",
    "visual_attention.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume your input image is a PyTorch tensor with shape [3, H, W]\n",
    "# for RGB channels and height/width dimensions respectively.\n",
    "# Make sure your image is normalized (values between 0 and 1) and\n",
    "# is of the correct input size for the model.\n",
    "input_image = cv2.imread(dataset_base_path + 'images/valA/' + x_valid_img[idx])\n",
    "input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "input_image = cv2.resize(input_image, (224, 224))\n",
    "input_image = input_image / 255.0\n",
    "input_image = input_image.transpose((2, 0, 1))\n",
    "\n",
    "# Add an extra batch dimension\n",
    "input_image = torch.unsqueeze(torch.from_numpy(input_image), 0)\n",
    "input_image = input_image.to(device)\n",
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model returns a list of dictionaries for each image in the batch\n",
    "# (even if there's just one). Each dict contains the bounding boxes \n",
    "# (\"boxes\"), labels (\"labels\"), and scores (\"scores\") for each detected object.\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "    # Extract the feature maps from the backbone of the Faster R-CNN\n",
    "    # which will be your image region feature V. \n",
    "    # The features are a tensor of shape [N, C, H, W] where N is the number\n",
    "    # of images (1 in this case), C is the number of channels, and H and W are \n",
    "    # the height and width of the feature map respectively.\n",
    "\n",
    "    # odict_keys(['0', '1', '2', '3', 'pool'])\n",
    "    v_feature = model.backbone(input_image)['0']\n",
    "    # Apply visual attention\n",
    "    # v_feature = v_feature.to(device)\n",
    "    print(v_feature.shape)\n",
    "    v_feature_att = visual_attention(v_feature)\n",
    "\n",
    "# Extract the bounding boxes and scores\n",
    "boxes = output[0]['boxes']\n",
    "scores = output[0]['scores']\n",
    "\n",
    "# Filter out detections with a score below some threshold, if necessary\n",
    "threshold = 0.5\n",
    "indices = scores > threshold\n",
    "boxes = boxes[indices]\n",
    "\n",
    "# The bounding boxes are in the format [x0, y0, x1, y1], where (x0, y0)\n",
    "# is the top-left corner of the box and (x1, y1) is the bottom-right.\n",
    "# convert these to a spatial coordinate feature C as needed.\n",
    "\n",
    "v_feature.shape, v_feature_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'boxes' contains the bounding box coordinates extracted from Faster R-CNN\n",
    "# and 'input_image' is original input image\n",
    "\n",
    "# Get the height and width of the image\n",
    "_, _, height, width = input_image.shape\n",
    "\n",
    "# Normalize the bounding box coordinates\n",
    "boxes_normalized = boxes / torch.tensor([width, height, width, height])\n",
    "\n",
    "# Compute the width and height of each box\n",
    "boxes_width_height = boxes_normalized[:, 2:] - boxes_normalized[:, :2]\n",
    "\n",
    "# Concatenate the normalized coordinates and the widths/heights to form the spatial features\n",
    "c_feature = torch.cat([boxes_normalized, boxes_width_height], dim=1)\n",
    "c_feature.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.hidden_dim = hidden_dim  # Storing hidden_dim for later use\n",
    "        \n",
    "    def forward(self, texts):\n",
    "        embedded = self.embedding(texts)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "\n",
    "        # Since the GRU is bidirectional, the hidden states from the two directions are stacked on top of each other.\n",
    "        # We need to add them together to get a single hidden state for each time step.\n",
    "        outputs = outputs[:, :, :self.hidden_dim] + outputs[:, :, self.hidden_dim:]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TextEmbedding model\n",
    "vocab_size = len(vocab)  # Your actual vocabulary size\n",
    "embed_dim = 300  # Embedding dimension as per the paper\n",
    "hidden_dim = 256  # This is a parameter that you can tune\n",
    "num_layers = 1  # This is also a tunable parameter\n",
    "pad_idx = vocab.index(\"<PAD>\")  # Get the index of the padding token\n",
    "\n",
    "model = TextEmbedding(vocab_size, embed_dim, hidden_dim, num_layers, pad_idx)\n",
    "\n",
    "# Now, let's test the model with some an input from data\n",
    "example_input = torch.tensor(x_valid_txt[0]).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "output = model(example_input)\n",
    "\n",
    "print(output.shape)  # Expected: [batch_size, sequence_length, hidden_dim]\n",
    "\n",
    "# Instantiate the Attention module\n",
    "feature_dim = 256  # The dimension of your features\n",
    "attention_model = Attention(feature_dim)\n",
    "\n",
    "# Pass the example features through the Attention module\n",
    "output = attention_model(output)\n",
    "\n",
    "print(output.shape)  # Expected: [batch_size, sequence_length, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding box\n",
    "def get_bbox(obj):\n",
    "    sizes = {\n",
    "        \"large\": 0.7,\n",
    "        \"small\": 0.35\n",
    "    }\n",
    "    bbox = []\n",
    "    if obj['shape'] == 'sphere':\n",
    "        bbox = [\n",
    "                obj['pixel_coords'][0] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 600, \n",
    "                obj['pixel_coords'][1] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 600, \n",
    "                (sizes[obj['size']] / obj['pixel_coords'][2]) * 600 * 2\n",
    "                , (sizes[obj['size']] / obj['pixel_coords'][2]) * 600 * 2]\n",
    "    elif obj['shape'] == 'cylinder':\n",
    "        bbox = [\n",
    "                obj['pixel_coords'][0] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 600, \n",
    "                obj['pixel_coords'][1] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 750, \n",
    "                (sizes[obj['size']] / obj['pixel_coords'][2]) * 600 * 2\n",
    "                , (sizes[obj['size']] / obj['pixel_coords'][2]) * 750 * 2]\n",
    "    else:\n",
    "        bbox = [\n",
    "                obj['pixel_coords'][0] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 800, \n",
    "                obj['pixel_coords'][1] - (sizes[obj['size']] / obj['pixel_coords'][2]) * 800, \n",
    "                (sizes[obj['size']] / obj['pixel_coords'][2]) * 800 * 2\n",
    "                , (sizes[obj['size']] / obj['pixel_coords'][2]) * 800 * 2]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from a scene\n",
    "def scene_extractor(scene):\n",
    "    \"\"\"\n",
    "    Extracts the following information from a scene:\n",
    "        - image_filename\n",
    "        - objects\n",
    "        - split\n",
    "        - image_index\n",
    "        - relationships\n",
    "        - directions\n",
    "    \"\"\"\n",
    "    # add bounding boxes to objects\n",
    "    for obj in scene['objects']:\n",
    "        obj['bbox'] = get_bbox(obj)\n",
    "    return {\n",
    "        'image_filename': scene['image_filename'],\n",
    "        'objects': scene['objects'],\n",
    "        'split': scene['split'],\n",
    "        'image_index': scene['image_index'],\n",
    "        'relationships': scene['relationships'],\n",
    "        'directions': scene['directions']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, height, width,transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "        self.x.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        # sort y to match x\n",
    "        self.y.sort(key=lambda y: y['image_index'])\n",
    "\n",
    "        # classes: 0 index is reserved for background\n",
    "        # self.classes = ['background', 'cube', 'sphere', 'cylinder']\n",
    "        self.classes = ['cube', 'sphere', 'cylinder']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if \"val\" in self.x[idx]:\n",
    "            img = cv2.imread(dataset_base_path + 'images/valA/' + self.x[idx])\n",
    "        else:\n",
    "            img = cv2.imread(dataset_base_path + 'images/trainA/' + self.x[idx])\n",
    "\n",
    "        org_shape = img.shape\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img = cv2.resize(img, (self.width, self.height), cv2.INTER_AREA)\n",
    "        img = img / 255.0\n",
    "\n",
    "        x_scale = self.width / org_shape[1]\n",
    "        y_scale = self.height / org_shape[0]\n",
    "\n",
    "        scene = scene_extractor(self.y[idx])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in scene['objects']:\n",
    "            labels.append(self.classes.index(obj['shape']))\n",
    "            # boxes.append(obj['bbox'])\n",
    "            this_bbox = obj['bbox']\n",
    "            # sync bbox with resized image\n",
    "            x_min = this_bbox[0] * x_scale\n",
    "            y_min = this_bbox[1] * y_scale\n",
    "            x_max = (this_bbox[0] + this_bbox[2]) * x_scale\n",
    "            y_max = (this_bbox[1] + this_bbox[3]) * y_scale\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "            # # normalize to 0-1\n",
    "            # bboxes = [x_min / self.width, y_min / self.height, x_max / self.width, y_max / self.height]\n",
    "            # # < 0 to 0 and > 1 to 1\n",
    "            # bboxes = [0 if b < 0 else b for b in bboxes]\n",
    "            # bboxes = [1 if b > 1 else b for b in bboxes]\n",
    "            # boxes.append(bboxes)\n",
    "\n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # getting the areas of the boxes\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = img,\n",
    "                            bboxes = target['boxes'],\n",
    "                            labels = labels)\n",
    "            img = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        img = img.transpose(2, 0, 1)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bounding boxes\n",
    "def plot_bbox(img, target):\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    classes = ['cube', 'sphere', 'cylinder']\n",
    "    for i in range(len(target['boxes'])):        \n",
    "        plt.gcf().gca().add_artist(plt.Rectangle(\n",
    "            (target['boxes'][i][0], target['boxes'][i][1]), \n",
    "            target['boxes'][i][2] - target['boxes'][i][0], \n",
    "            target['boxes'][i][3] - target['boxes'][i][1], \n",
    "            color='red', fill=False)\n",
    "        )\n",
    "        plt.text(target['boxes'][i][0], target['boxes'][i][1], classes[target['labels'][i]], color='white')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataset and defined transformations\n",
    "# dataset = CLEVRDataset(x_train, y_train, 224, 224, transforms=get_transform(train=True))\n",
    "# dataset_valid = CLEVRDataset(x_val, y_val, 224, 224, transforms=get_transform(train=False))\n",
    "dataset_train_img = CLEVRDataset(x_train_img, y_train_img, 224, 224)\n",
    "dataset_valid_img = CLEVRDataset(x_valid_img, y_valid_img, 224, 224)\n",
    "dataset_valid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset_valid_img[random.randint(0, len(y_valid_img))]\n",
    "print(img.shape)\n",
    "print(target)\n",
    "plot_bbox(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset_valid_img.classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train_img, batch_size=10, shuffle=True, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_valid = torch.utils.data.DataLoader(\n",
    "    dataset_valid_img, batch_size=10, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "data_loader_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0eb280f2a99e787a1cb7c4382dc3dc8031a887c8fd3d22c9b74e1e2b688607e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
